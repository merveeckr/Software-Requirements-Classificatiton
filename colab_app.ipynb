import gradio as gr

DEFAULT_MODEL = "dbmdz/bert-base-turkish-uncased"

def process_csv(csv_file, model_name, threshold, weights_file):
    try:
        df = pd.read_csv(csv_file.name)
    except Exception as e:
        return None, f"CSV okunamadı: {e}"
    if TEXT_COL not in df.columns:
        return None, f"CSV içinde '{TEXT_COL}' kolonu yok"
    texts = df[TEXT_COL].fillna("").astype(str).tolist()
    preds = predict(
        texts=texts,
        model_name=model_name or DEFAULT_MODEL,
        max_len=128,
        threshold=float(threshold),
        weights_path=weights_file.name if weights_file else None,
    )
    ai_df = pd.DataFrame(preds, columns=[f"AI_{c}" for c in LABEL_COLS])
    out = pd.concat([df.reset_index(drop=True), ai_df], axis=1)
    out["Eksikler_AI"] = [
        [c for c in LABEL_COLS if int(row[f"AI_{c}"]) == 0]
        for _, row in out.iterrows()
    ]
    out_path = "/content/sonuc.csv"
    out.to_csv(out_path, index=False)
    return out_path, out


def suggest_row(model_id, requirement, missing_str):
    missing = [x.strip() for x in (missing_str or "").split(',') if x.strip()]
    gen = build_suggestion_model(model_id) if model_id else None
    text = suggest(gen, requirement, missing)
    return text or "(Öneri üretilemedi)"

with gr.Blocks() as demo:
    gr.Markdown("# Gereksinim Analizi — CSV + Öneri")
    with gr.Tab("CSV Analiz"):
        csv_in = gr.File(label="CSV yükleyin")
        with gr.Row():
            model_in = gr.Textbox(label="BERT Model (HF ID)", value=DEFAULT_MODEL)
            thr_in = gr.Slider(0.05, 0.95, value=0.5, step=0.05, label="Eşik")
            weights_in = gr.File(label="Ağırlık dosyası (best_model.pt)")
        run_btn = gr.Button("Analiz Et")
        csv_out = gr.File(label="Çıktı CSV")
        df_out = gr.Dataframe(label="Önizleme", wrap=True)
        run_btn.click(process_csv, inputs=[csv_in, model_in, thr_in, weights_in], outputs=[csv_out, df_out])
    with gr.Tab("Satır İyileştirme (Gemma opsiyonel)"):
        req_in = gr.Textbox(label="Gereksinim")
        miss_in = gr.Textbox(label="Eksikler (virgülle)", placeholder="Unambiguous, Verifiable")
        modelid_in = gr.Textbox(label="HF Model ID (ör. google/gemma-2b-it)")
        sug_btn = gr.Button("Öneri Üret")
        sug_out = gr.Textbox(label="AI Önerisi")
        sug_btn.click(suggest_row, inputs=[modelid_in, req_in, miss_in], outputs=[sug_out])

demo.launch(share=True)import os
from typing import List
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from transformers import AutoTokenizer, AutoModel, pipeline

LABEL_COLS = ['Appropriate','Complete','Conforming','Correct','Feasible','Necessary','Singular','Unambiguous','Verifiable']
TEXT_COL = 'Requirement'

class ReqDataset(Dataset):
    def __init__(self, texts: List[str], tokenizer, max_len: int):
        self.texts = texts
        self.tokenizer = tokenizer
        self.max_len = max_len
    def __len__(self):
        return len(self.texts)
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_len,
            return_tensors='pt'
        )
        return {"input_ids": encoding["input_ids"].squeeze(0),
                "attention_mask": encoding["attention_mask"].squeeze(0)}

class BertBiLSTMCNN(nn.Module):
    def __init__(self, bert_model_name: str, lstm_hidden=256, lstm_layers=1, cnn_out_channels=128, kernel_sizes=(2,3,4), dropout=0.3, num_labels=9):
        super().__init__()
        self.bert = AutoModel.from_pretrained(bert_model_name)
        bert_hidden = self.bert.config.hidden_size
        self.lstm = nn.LSTM(input_size=bert_hidden, hidden_size=lstm_hidden, num_layers=lstm_layers,
                            batch_first=True, bidirectional=True, dropout=0.0 if lstm_layers==1 else dropout)
        self.convs = nn.ModuleList([
            nn.Conv1d(in_channels=2*lstm_hidden, out_channels=cnn_out_channels, kernel_size=k)
            for k in kernel_sizes
        ])
        self.global_pool = nn.AdaptiveMaxPool1d(1)
        total_conv_out = cnn_out_channels * len(kernel_sizes)
        self.classifier = nn.Sequential(
            nn.Linear(total_conv_out, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, num_labels)
        )
    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)
        hidden_states = outputs.last_hidden_state
        lstm_out, _ = self.lstm(hidden_states)
        x = lstm_out.permute(0, 2, 1)
        conv_outs = []
        for conv in self.convs:
            c = conv(x)
            p = self.global_pool(c)
            conv_outs.append(p.squeeze(-1))
        cat = torch.cat(conv_outs, dim=1)
        logits = self.classifier(cat)
        return logits

@torch.no_grad()
def predict(texts: List[str], model_name: str, max_len: int, threshold: float, weights_path: str|None=None) -> np.ndarray:
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = BertBiLSTMCNN(bert_model_name=model_name, num_labels=len(LABEL_COLS)).to(device)
    if weights_path and os.path.exists(weights_path):
        model.load_state_dict(torch.load(weights_path, map_location=device), strict=False)
    model.eval()
    ds = ReqDataset(texts, tokenizer, max_len)
    dl = DataLoader(ds, batch_size=16, shuffle=False)
    all_preds = []
    for batch in dl:
        logits = model(batch['input_ids'].to(device), batch['attention_mask'].to(device))
        probs = torch.sigmoid(logits).cpu().numpy()
        preds = (probs >= threshold).astype(int)
        all_preds.append(preds)
    return np.vstack(all_preds) if all_preds else np.zeros((0, len(LABEL_COLS)), dtype=int)


def build_suggestion_model(hf_model_id: str|None):
    if not hf_model_id:
        return None
    try:
        gen = pipeline('text-generation', model=hf_model_id, device=0 if torch.cuda.is_available() else -1, return_full_text=False)
        return gen
    except Exception:
        return None


def suggest(gen, requirement: str, missing: List[str]) -> str:
    if gen is None or not missing:
        return ""
    prompt = (
        "Yalnızca TÜRKÇE yaz. Girişi tekrar etme.\n"
        "Belirsizliği kaldır, doğrulanabilir ve ölçülebilir hale getir.\n"
        f"Eksikleri gider: {', '.join(missing)}\n\n"
        f"Gereksinim: {requirement}\n"
        "ÇIKTI: Yalnızca tek satır 'İyileştirilmiş gereksinim: <cümle>' yaz."
    )
    out = gen(prompt, max_new_tokens=96, do_sample=False, num_beams=4)
    text = out[0].get('generated_text','').strip()
    low = text.lower()
    if 'iyileştirilmiş gereksinim' in low:
        try:
            start = low.index('iyileştirilmiş gereksinim')
            text = text[start:].split('\n',1)[0]
            if ':' in text:
                text = text.split(':',1)[1].strip()
        except Exception:
            pass
    return text
# Install deps
!pip -q install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
!pip -q install transformers==4.42.4 accelerate==0.33.0 gradio==4.41.0 pandas==2.2.2 numpy==1.26.4 scikit-learn tiktoken protobuf# Gereksinim Analizi (BERTürk + Öneri) — Colab/Gradio

Bu not defteri, yerel kurulum olmadan Google Colab üzerinde bir arayüz sağlar:
- CSV yükleyin, çoklu-etiket tahminleri alın (AI_ sütunları) ve eksikleri görün
- (Opsiyonel) `best_model.pt` ağırlıklarını yükleyip kendi modelinizi kullanın
- (Opsiyonel) eksik etiketlere göre Türkçe iyileştirilmiş gereksinim önerisi üretin

Çalıştırma adımları:
1. Aşağıdaki kurulum hücresini çalıştırın
2. "Uygulamayı başlat" hücresini çalıştırın ve çıkan Gradio bağlantısına tıklayın

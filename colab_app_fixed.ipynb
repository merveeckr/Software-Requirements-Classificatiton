import os
from typing import List
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from transformers import AutoTokenizer, AutoModel, pipeline

LABEL_COLS = ['Appropriate','Complete','Conforming','Correct','Feasible','Necessary','Singular','Unambiguous','Verifiable']
TEXT_COL = 'Requirement'

class ReqDataset(Dataset):
    def __init__(self, texts: List[str], tokenizer, max_len: int):
        self.texts = texts
        self.tokenizer = tokenizer
        self.max_len = max_len
    def __len__(self):
        return len(self.texts)
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_len,
            return_tensors='pt'
        )
        return {"input_ids": encoding["input_ids"].squeeze(0),
                "attention_mask": encoding["attention_mask"].squeeze(0)}

class BertBiLSTMCNN(nn.Module):
    def __init__(self, bert_model_name: str, lstm_hidden=256, lstm_layers=1, cnn_out_channels=128, kernel_sizes=(2,3,4), dropout=0.3, num_labels=9):
        super().__init__()
        self.bert = AutoModel.from_pretrained(bert_model_name)
        bert_hidden = self.bert.config.hidden_size
        self.lstm = nn.LSTM(input_size=bert_hidden, hidden_size=lstm_hidden, num_layers=lstm_layers,
                            batch_first=True, bidirectional=True, dropout=0.0 if lstm_layers==1 else dropout)
        self.convs = nn.ModuleList([
            nn.Conv1d(in_channels=2*lstm_hidden, out_channels=cnn_out_channels, kernel_size=k)
            for k in kernel_sizes
        ])
        self.global_pool = nn.AdaptiveMaxPool1d(1)
        total_conv_out = cnn_out_channels * len(kernel_sizes)
        self.classifier = nn.Sequential(
            nn.Linear(total_conv_out, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, num_labels)
        )
    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)
        hidden_states = outputs.last_hidden_state
        lstm_out, _ = self.lstm(hidden_states)
        x = lstm_out.permute(0, 2, 1)
        conv_outs = []
        for conv in self.convs:
            c = conv(x)
            p = self.global_pool(c)
            conv_outs.append(p.squeeze(-1))
        cat = torch.cat(conv_outs, dim=1)
        logits = self.classifier(cat)
        return logits

@torch.no_grad()
def predict(texts: List[str], model_name: str, max_len: int, threshold: float, weights_path: str|None=None) -> np.ndarray:
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = BertBiLSTMCNN(bert_model_name=model_name, num_labels=len(LABEL_COLS)).to(device)
    if weights_path and os.path.exists(weights_path):
        model.load_state_dict(torch.load(weights_path, map_location=device), strict=False)
    model.eval()
    ds = ReqDataset(texts, tokenizer, max_len)
    dl = DataLoader(ds, batch_size=16, shuffle=False)
    all_preds = []
    for batch in dl:
        logits = model(batch['input_ids'].to(device), batch['attention_mask'].to(device))
        probs = torch.sigmoid(logits).cpu().numpy()
        preds = (probs >= threshold).astype(int)
        all_preds.append(preds)
    return np.vstack(all_preds) if all_preds else np.zeros((0, len(LABEL_COLS)), dtype=int)


def build_suggestion_model(hf_model_id: str|None):
    if not hf_model_id:
        return None
    try:
        gen = pipeline('text-generation', model=hf_model_id, device=0 if torch.cuda.is_available() else -1, return_full_text=False)
        return gen
    except Exception:
        return None


def suggest(gen, requirement: str, missing: List[str]) -> str:
    if gen is None or not missing:
        return ""
    prompt = (
        "Yalnızca TÜRKÇE yaz. Girişi tekrar etme.\n"
        "Belirsizliği kaldır, doğrulanabilir ve ölçülebilir hale getir.\n"
        f"Eksikleri gider: {', '.join(missing)}\n\n"
        f"Gereksinim: {requirement}\n"
        "ÇIKTI: Yalnızca tek satır 'İyileştirilmiş gereksinim: <cümle>' yaz."
    )
    out = gen(prompt, max_new_tokens=96, do_sample=False, num_beams=4)
    text = out[0].get('generated_text','').strip()
    low = text.lower()
    if 'iyileştirilmiş gereksinim' in low:
        try:
            start = low.index('iyileştirilmiş gereksinim')
            text = text[start:].split('\n',1)[0]
            if ':' in text:
                text = text.split(':',1)[1].strip()
        except Exception:
            pass
    return text# Install deps
!pip -q install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
!pip -q install transformers==4.42.4 accelerate==0.33.0 gradio==4.41.0 pandas==2.2.2 numpy==1.26.4 scikit-learn tiktoken protobuf# Gereksinim Analizi (BERTürk + Öneri) — Colab/Gradio

Yerel kurulum gerekmeden Google Colab üzerinde bir arayüz sağlar:
- CSV yükleyin, çoklu-etiket tahminleri alın (AI_ sütunları) ve eksikleri görün
- (Opsiyonel) `best_model.pt` yükleyip kendi eğitilmiş modelinizi kullanın
- (Opsiyonel) eksik etiketlere göre Türkçe iyileştirilmiş gereksinim önerisi üretin

Adımlar:
1. Aşağıdaki kurulum hücresini çalıştırın
2. Son hücreyi çalıştırın ve çıkan Gradio bağlantısına tıklayın